# notes
    """
    find spaces
    define words by the number space they appear before
    search string for matches
    take distribution of word vs index order
    search string for final punctuation
    split string by sentences with words separated out
    take distribution of word vs sentence order
    check for word groups that commute together

    other: temporal markers, aspect markers, function markers
    mood tags: casual, happy, sad, funny/witty, light-hearted, stern, authoritative, friendly, angry, surprised, confused, snide, poetic, non-cooperative, sarcastic/ironic, post-ironic, meta-ironic, 

    take distribution of phoneme vs sentence order
    note any phonemes that move together often and phonemes that never get paired in a certain order

    plot words as nodes and each sentence is a path through the graph
    to start, each sentence is a new graph and after parsing the info, the graph gets condensed
    keep a large-scale graph for structural objects and a more precise one for individual word choice
    """

# lexicon is a table of all words with accompanying traits



# word parts

    class definitions:

        def __init__(self, def_code, definition):
            self.def_code = def_code
            self.definition = definition

        def def_lookup(self):
            print(self.definition)




    class phonemes:

        def __init__:

    class mood:

        def __init__:

    class subclass:

        def __init__:

    class superclass:

        def __init__:

    class word:

        def __init__(self, phonemes, speech_code, def_code, mood_code, sub_code, super_code):

            self.part_of_speech = part_of_speech
            self.phonemes = phonemes
            self.definition = definition
            self.mood = mood
            self.subclass = subclass
            self.superclass = superclass

        def lookup(self):


                part_of_speech = parts_of_speech[speech_code.word(str)]
                speech_code = int

                pronounce(word) = phonemes[phoneme_code.word(str)]
                phoneme_code = []

                defintion(word) = definitions[def_code.word(str)]
                def_code = 

                mood(word) = mood_markers[mood_code.word(str)]
                mood_code = 

                subclass(word) = subclasses[sub_code.word(str)]
                sub_code = 

                superclass(word) = superclasses[super_code.word(str)]
                super_code = 



    parts_of_speech = ['noun/adj', 'pronoun', 'adv', 'verb', 'prep', 'conj', 'int', 'part']





    lexicon(word) = [word(), phonemes.word(), part_of_speech.word(), definition.word(), mood_marker.word(), subclass.word(), superclass.word()]

# definition generation

    for word in words:

        if word in lexicon:
            # dadada
        else:
            lexicon.append(word)

def dict_lookup(word):
    print()


"""

import pandas as pd

txt = example.txt
wordlist() = str.split(txt)
word_count() = len.str.split(txt)

lexicon = pd.DataFrame()

phoneme = str
lines = txt.readlines()
space = " "



pointers for other groups

    speech_code = int
    phoneme_code = int
    def_code = int
    mood_code = int


                    [ 0, 1, 2, 3, 4, 5, 6, 7]



# wordfinder inputs a string and outputs an array of words and number of occurences

def wordfinder(txt):
    # numbering all the words and spaces
        for i, ltr in enumerate(line):
            if ltr == space:
                yield i
                i+= 1
        
            else:

                spaces() = spaces.append(space[i])
                space[i] = line.find(space)
                space
            
            
                return enumerate(line)
                return spaces

                # This requires you to add a space to the beginning of the line to act as the 0th space

    words() = pd.DataFrame([word],[count])
    word(n) = line[spaces[n-1] << str_index << spaces[space[n]]]

    # creating the wordcount array

        if word in words:
                count.word(n) += 1  
            else:
                words.append(word)
                count.word(n) = 1

                return words

                words

def parser(txt, words)
    for word in words:
        if word not in lexicon:
            lexicon.append(word)
        if word in lexicon:
            # dadada

def phrasefinder(txt)
    # definitions
        comma = ","
        period = "."
        semicolon = ";"
        colon = ":"
        l_parenthesis = "("
        r_parenthesis = ")"
        l_bracket = "["
        r_bracket = "]"

        punctuation = [comma, period, semicolon, colon, l_parenthesis, r_parenthesis, l_bracket, r_bracket]
        phrase_type = [Noun, Verb, Adject, Adverb, Gerund, Infinitive, Prepositional, Absolute, Appositive]

    #
        for i, ltr in enumerate(line): 
            if ltr in punctuation:
                yield i
                i+= 1
        
            else:

                i+= 1
            
                return enumerate(line)
                return spaces

                # This requires you to add a period to the beginning of the line to act as the 0th punctuation


for word in lexicon:


word_data 
